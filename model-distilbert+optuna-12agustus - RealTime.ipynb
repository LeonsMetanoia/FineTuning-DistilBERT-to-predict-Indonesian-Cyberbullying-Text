{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "da2f7440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU tersedia: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ GPU tersedia:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"‚ùå GPU tidak tersedia. Menggunakan CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6ca8273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161a8de",
   "metadata": {},
   "source": [
    "# Import Libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "257c5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    DistilBertForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from datasets import DatasetDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141ed5e",
   "metadata": {},
   "source": [
    "# LOAD Dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0e3467d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Test DistilBERT+Optuna Lokal/combined_dataset.csv')\n",
    "df = df[['encoded_label', 'clean_text']].dropna()\n",
    "\n",
    "# Pastikan label integer\n",
    "df['encoded_label'] = df['encoded_label'].astype(int)\n",
    "\n",
    "# Convert ke HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ab574952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "\n",
    "len(set(df[\"encoded_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e206b382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['encoded_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "06a5b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"encoded_label\"] = df[\"encoded_label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3365aee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['encoded_label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980114cf",
   "metadata": {},
   "source": [
    "# Split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "71286ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'test': dataset['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd376c5",
   "metadata": {},
   "source": [
    "# 2. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9f099d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1652/1652 [00:00<00:00, 18835.07 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 414/414 [00:00<00:00, 17302.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Model & Tokenizer\n",
    "model_name = \"cahya/distilbert-base-indonesian\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cahya/distilbert-base-indonesian\")\n",
    "\n",
    "# Tokenisasi\n",
    "def tokenize_fn(batch):\n",
    "    tokens = tokenizer(batch['clean_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokens[\"labels\"] = batch[\"encoded_label\"]  # langsung int, jangan dibungkus list\n",
    "    return tokens\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f03b0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(set(df[\"encoded_label\"]))\n",
    "def model_init():\n",
    "    return DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"cahya/distilbert-base-indonesian\",\n",
    "        num_labels=num_labels\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "df8f248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['encoded_label', 'clean_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1652\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['encoded_label', 'clean_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 414\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538bccd",
   "metadata": {},
   "source": [
    "# 3. Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "86c65378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]  # ambil logits saja\n",
    "    \n",
    "    logits = np.array(logits)\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    labels = np.array(pred.label_ids).flatten()\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted'),\n",
    "        'precision': precision_score(labels, preds, average='weighted'),\n",
    "        'recall': recall_score(labels, preds, average='weighted')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdfa5c",
   "metadata": {},
   "source": [
    "# 4. Optuna Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "234868d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def objective(trial):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        per_device_train_batch_size=trial.suggest_categorical(\"train_batch_size\", [8, 16, 32]),\n",
    "        per_device_eval_batch_size=trial.suggest_categorical(\"eval_batch_size\", [8, 16, 32]),\n",
    "        num_train_epochs=trial.suggest_int(\"epochs\", 2, 5),\n",
    "        weight_decay=trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        logging_dir=\"./logs\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    return metrics[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a7b0a",
   "metadata": {},
   "source": [
    "# 5. Jalankan Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d275c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 10:47:41,030] A new study created in memory with name: no-name-250cb62b-ce5a-4708-8fc1-802472dd6b55\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_34064\\396335439.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [416/416 07:16, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.368184</td>\n",
       "      <td>0.857488</td>\n",
       "      <td>0.855121</td>\n",
       "      <td>0.878227</td>\n",
       "      <td>0.857488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276968</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.898558</td>\n",
       "      <td>0.898942</td>\n",
       "      <td>0.898551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.386984</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.907857</td>\n",
       "      <td>0.912706</td>\n",
       "      <td>0.908213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.391386</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.907961</td>\n",
       "      <td>0.911179</td>\n",
       "      <td>0.908213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 10:55:51,238] Trial 0 finished with value: 0.9079609406480765 and parameters: {'learning_rate': 1.9775290578318554e-05, 'train_batch_size': 16, 'eval_batch_size': 16, 'epochs': 4, 'weight_decay': 0.293847716633629}. Best is trial 0 with value: 0.9079609406480765.\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_34064\\396335439.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='520' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [520/520 01:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.260903</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888834</td>\n",
       "      <td>0.889154</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.310407</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.898522</td>\n",
       "      <td>0.900006</td>\n",
       "      <td>0.898551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.418202</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.912921</td>\n",
       "      <td>0.914342</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.466745</td>\n",
       "      <td>0.903382</td>\n",
       "      <td>0.903065</td>\n",
       "      <td>0.907016</td>\n",
       "      <td>0.903382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.497248</td>\n",
       "      <td>0.905797</td>\n",
       "      <td>0.905461</td>\n",
       "      <td>0.909848</td>\n",
       "      <td>0.905797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 10:57:27,745] Trial 1 finished with value: 0.9129214311649293 and parameters: {'learning_rate': 4.5969806994022144e-05, 'train_batch_size': 16, 'eval_batch_size': 32, 'epochs': 5, 'weight_decay': 0.2839726560296282}. Best is trial 1 with value: 0.9129214311649293.\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_34064\\396335439.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 09:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.434567</td>\n",
       "      <td>0.792271</td>\n",
       "      <td>0.786512</td>\n",
       "      <td>0.822198</td>\n",
       "      <td>0.792271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.336727</td>\n",
       "      <td>0.850242</td>\n",
       "      <td>0.849113</td>\n",
       "      <td>0.858411</td>\n",
       "      <td>0.850242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.305126</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.861976</td>\n",
       "      <td>0.864567</td>\n",
       "      <td>0.862319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.308912</td>\n",
       "      <td>0.879227</td>\n",
       "      <td>0.878683</td>\n",
       "      <td>0.884144</td>\n",
       "      <td>0.879227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.302730</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.883677</td>\n",
       "      <td>0.887473</td>\n",
       "      <td>0.884058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 11:07:42,864] Trial 2 finished with value: 0.8836774143348226 and parameters: {'learning_rate': 1.1443391799396131e-05, 'train_batch_size': 32, 'eval_batch_size': 16, 'epochs': 5, 'weight_decay': 0.2576330261623983}. Best is trial 1 with value: 0.9129214311649293.\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_34064\\396335439.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='414' max='414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [414/414 04:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.367107</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.869852</td>\n",
       "      <td>0.855072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.352009</td>\n",
       "      <td>0.881643</td>\n",
       "      <td>0.881220</td>\n",
       "      <td>0.885393</td>\n",
       "      <td>0.881643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 11:13:37,529] Trial 3 finished with value: 0.8812198935076548 and parameters: {'learning_rate': 1.4582519133347599e-05, 'train_batch_size': 8, 'eval_batch_size': 16, 'epochs': 2, 'weight_decay': 0.04081981526171976}. Best is trial 1 with value: 0.9129214311649293.\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_34064\\396335439.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 01:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448791</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.790018</td>\n",
       "      <td>0.837070</td>\n",
       "      <td>0.797101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.312081</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.861450</td>\n",
       "      <td>0.869233</td>\n",
       "      <td>0.862319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.297190</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888733</td>\n",
       "      <td>0.890078</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.309911</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.883609</td>\n",
       "      <td>0.888224</td>\n",
       "      <td>0.884058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.305380</td>\n",
       "      <td>0.896135</td>\n",
       "      <td>0.895969</td>\n",
       "      <td>0.897581</td>\n",
       "      <td>0.896135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 11:15:22,771] Trial 4 finished with value: 0.8959694029151666 and parameters: {'learning_rate': 1.3953319112264623e-05, 'train_batch_size': 32, 'eval_batch_size': 32, 'epochs': 5, 'weight_decay': 0.11291836859889037}. Best is trial 1 with value: 0.9129214311649293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "FrozenTrial(number=1, state=1, values=[0.9129214311649293], datetime_start=datetime.datetime(2025, 8, 12, 10, 55, 51, 238879), datetime_complete=datetime.datetime(2025, 8, 12, 10, 57, 27, 745178), params={'learning_rate': 4.5969806994022144e-05, 'train_batch_size': 16, 'eval_batch_size': 32, 'epochs': 5, 'weight_decay': 0.2839726560296282}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=5e-05, log=True, low=1e-05, step=None), 'train_batch_size': CategoricalDistribution(choices=(8, 16, 32)), 'eval_batch_size': CategoricalDistribution(choices=(8, 16, 32)), 'epochs': IntDistribution(high=5, log=False, low=2, step=1), 'weight_decay': FloatDistribution(high=0.3, log=False, low=0.0, step=None)}, trial_id=1, value=None)\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5)  # Ubah n_trials untuk eksplor lebih banyak\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0efc5",
   "metadata": {},
   "source": [
    "# Save The Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "32306042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model disimpan di folder: ./best_model\n"
     ]
    }
   ],
   "source": [
    "# Setelah training selesai dan sudah ada 'model' hasil training\n",
    "output_dir = \"./best_model\"\n",
    "\n",
    "# Simpan model dan tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model disimpan di folder: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5b90c",
   "metadata": {},
   "source": [
    "# Pengujian tingkat kecepatan dalam melakukan prediksi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c1ddd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_23180\\1553166491.py:17: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  df = pd.read_csv(\"D:\\Project-Prep\\Test DistilBERT+Optuna Lokal\\cleantext_CB.csv\")\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è± Total waktu: 0.0488 detik untuk 500 teks\n",
      "üìä Rata-rata per teks: 0.10 ms\n",
      "‚ö° Throughput: 10236.86 teks/detik\n",
      "üéØ Latency real-time 1 teks: 6.23 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# === 1. Load model dan tokenizer ===\n",
    "model_path = \"./best_model\"  # path model yang sudah kamu save\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Pastikan pakai GPU kalau ada\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# === 2. Load dataset (contoh) ===\n",
    "# Misalnya file CSV berisi kolom 'comment'\n",
    "df = pd.read_csv(\"D:\\Project-Prep\\Test DistilBERT+Optuna Lokal\\cleantext_CB.csv\")\n",
    "\n",
    "# Ambil sample 500 komentar random untuk pengujian\n",
    "sample_texts = df[\"comment\"].sample(500, random_state=42).tolist()\n",
    "\n",
    "# === 3. Uji waktu inference (batch) ===\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Tokenisasi batch\n",
    "inputs = tokenizer(sample_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Matikan gradient untuk speed\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "# Hitung waktu selesai\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# === 4. Hitung metrik waktu ===\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_text = total_time / len(sample_texts)\n",
    "throughput = len(sample_texts) / total_time\n",
    "\n",
    "print(f\"‚è± Total waktu: {total_time:.4f} detik untuk {len(sample_texts)} teks\")\n",
    "print(f\"üìä Rata-rata per teks: {avg_time_per_text*1000:.2f} ms\")\n",
    "print(f\"‚ö° Throughput: {throughput:.2f} teks/detik\")\n",
    "\n",
    "# === 5. Uji waktu real-time untuk satu komentar ===\n",
    "test_comment = \"Goblok lu anjinggg!\"\n",
    "inputs = tokenizer(test_comment, return_tensors=\"pt\").to(device)\n",
    "\n",
    "start_rt = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "end_rt = time.perf_counter()\n",
    "\n",
    "print(f\"üéØ Latency real-time 1 teks: {(end_rt - start_rt)*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3522cf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Average latency per sample: 6.23 ms\n",
      "‚ö° Throughput: 160.55 samples/sec\n",
      "Total samples tested: 200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ====== 1. Load model & tokenizer ======\n",
    "model_path = \"./best_model\"  # ganti sesuai path model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()  # mode evaluasi\n",
    "\n",
    "# Gunakan GPU kalau ada\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ====== 2. Contoh dataset (ambil dari data test) ======\n",
    "# Misalnya kita sudah punya 'test_texts' dan 'test_labels'\n",
    "# Untuk contoh ini, saya buat dummy:\n",
    "test_texts = [\n",
    "    \"You are so stupid!\",\n",
    "    \"I hope you have a great day!\",\n",
    "    \"Nobody likes you, go away!\",\n",
    "    \"That's an amazing performance!\"\n",
    "] * 50  # gandakan untuk 200 sample\n",
    "\n",
    "# ====== 3. Sampling data (misalnya 200 sample) ======\n",
    "sample_size = 200\n",
    "texts_to_test = test_texts[:sample_size]\n",
    "\n",
    "# ====== 4. Ukur kecepatan ======\n",
    "latencies = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in texts_to_test:\n",
    "        start_time = time.time()\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        latencies.append(end_time - start_time)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "# ====== 5. Hasil perhitungan ======\n",
    "avg_latency = np.mean(latencies)  # rata-rata detik per sample\n",
    "throughput = 1 / avg_latency  # sample per detik\n",
    "\n",
    "print(f\"üìä Average latency per sample: {avg_latency*1000:.2f} ms\")\n",
    "print(f\"‚ö° Throughput: {throughput:.2f} samples/sec\")\n",
    "print(f\"Total samples tested: {len(texts_to_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
